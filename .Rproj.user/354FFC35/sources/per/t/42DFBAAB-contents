---
title: "Advance Regression Term Project"
author:
  - name: "Gerard Palomo & Juan Pablo Uphoff"
    affiliation:
      - affsuperscript: 1
        dptuniv: "Department of Statistics / University Carlos III of Madrid"
        address: >
          Calle Madrid 126 Getafe,
          28903,
          Spain.
#corrauthor:
#  url: https://www.uc3m.es
abstract: >
  Ordinary least squares (OLS) regression fits the mean and is sensitive to extreme observations, whereas quantile regression (e.g. median regression) fits conditional quantiles and tends to be more robust to outliers. We conduct a simulation study comparing OLS and median (50% quantile) regression under increasing levels of outlier contamination in the response variable. Synthetic data are generated from a simple linear model and a fraction of responses are replaced by large aberrant values. We fit both OLS and quantile regression and compare the resulting coefficient estimates. As expected, OLS estimates become severely biased in the presence of outliers, while the median regression estimates remain closer to the true values. These findings confirm that quantile regression can better resist outliers than OLS
keywords: [OLS, Quantile Regression]
JEL: [C15, C21]
acknowledgements: >
  This report was completed for the course **Advanced Regression and Prediction**, as part of the **MSc in Statistics for Data Science** at **University Carlos III of Madrid**.
journalinfo: "Working Paper, MSc in Statistics for Data Science, Carlos III University of Madrid"
#archive: "DOI: N/A (internal report)"
date: "`r format(Sys.time(), '%Y %B %d')`"
lang: en-US
#otherlangs: [fr-FR,it]
keywordlabel: Keywords
JELlabel: JEL
#acknowledgementslabel: Acknowledgements
#corrauthorlabel: Corresponding author
bibliography: references.bib
biblio-style: apalike
urlcolor: blue
preamble: >
  \hyphenation{quan-tile re-gres-sion out-li-ers het-ero-ske-das-tic}
always_allow_html: yes
csquotes: true
output:
  bookdown::pdf_book:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: yes
    fontsize: 10pt
    toc: false
    toc_depth: 3
    number_sections: true
    fig_caption: true
    highlight: tango
    geometry: margin=1in
    linestretch: 1.5
    mainfont: "Times New Roman"  
    monofont: "Courier New"  
    template: latex/template.tex
---
```{r}
#| label: DoNotModify
#| include: false
### Utilities. Do not modify.
install_packages <- function(pkgs) {
  installed <- rownames(installed.packages())
  to_install <- setdiff(pkgs, installed)
  if (length(to_install) > 0) install.packages(to_install)
}
install_packages(c("bookdown", "tidyverse", "quantreg", "kableExtra", "gridExtra", "ragg"))

library(quantreg)
library(gridExtra)
library(kableExtra)
library(tidyverse)
library(tibble)


knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  include = FALSE,
  fig.align = 'center',
  out.width = "100%",
  fig.keep = "high",
  size = "scriptsize"
)
set.seed(666)
```

```{r}
#| label: Options
#| include: false
### Customized options for this document
# Add necessary packages here
packages <- c("tidyverse")
# Install them
install_packages(packages)

# knitr options
knitr::opts_chunk$set(
  cache = TRUE, # Cache chunk results
  include = TRUE, # Show/Hide chunks
  echo = TRUE, # Show/Hide code
  warning = FALSE, # Show/Hide warnings
  message = FALSE, # Show/Hide messages
  # Figure alignment and size
  fig.align = "center", out.width = "90%",
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
)
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)
knitr::opts_chunk$set(dev.args = list(bg = "transparent"))

# Random seed
set.seed(666)
```

```{r}
#| label: Code
#| include: false
#| echo: false

library(quantreg)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(kableExtra)
library(tibble)

# Fixed settings
set.seed(666)
beta0 <- 5; beta1 <- 1.5; beta2 <- -1.0
n_sim <- 100
outlier_frac <- 0.02
outlier_shift <- 30
tau_levels <- c(0.025, 0.25, 0.5, 0.75, 0.975)

# Simulation function
simulate_data <- function(n = 200, multivariate = FALSE, heterosk = FALSE) {
  x1 <- runif(n, 0, 10)
  x2 <- if (multivariate) runif(n, 0, 10) else rep(0, n)
  sd_e <- if (heterosk) (1 + 0.6*x1) else 2
  e <- rnorm(n, mean = 0, sd = sd_e)
  y <- beta0 + beta1 * x1 + beta2 * x2 + e
  data.frame(y = y, x1 = x1, x2 = x2) 
}

add_high_leverage_point <- function(data) {
  # Define a new x1 value far beyond the existing range
  new_x1 <- max(data$x1)
  new_x2 <- mean(data$x2) + 1  # Reasonable x2 value
  new_y <- max(data$y) * 20  # Extreme y value for high leverage

  # Create a new data point
  new_point <- data.frame(y = new_y, x1 = new_x1, x2 = new_x2)

  # Append it to the original dataset
  data_out <- rbind(data, new_point)
  return(data_out)
}

# Fit models function
fit_models <- function(data, multivariate = FALSE) {
  if (multivariate) {
    ols <- lm(y ~ x1 + x2, data = data)
    qrs <- lapply(tau_levels, function(tau) rq(y ~ x1 + x2, tau = tau, data = data))
  } else {
    ols <- lm(y ~ x1, data = data)
    qrs <- lapply(tau_levels, function(tau) rq(y ~ x1, tau = tau, data = data))
  }
  list(ols = ols, qrs = qrs)
}
# Different scenarios

multivariate_data = simulate_data(n=500, multivariate=TRUE, heterosk = TRUE)
multivariate_data_outlier = add_high_leverage_point(multivariate_data)

scenarios <- list(
  clean_multi = multivariate_data,
  outlier_multi = multivariate_data_outlier
)

# Fit models
models <- list(
  clean_multi = fit_models(scenarios$clean_multi, multivariate = TRUE),
  outlier_multi = fit_models(scenarios$outlier_multi, multivariate = TRUE)
)
```

# Introduction.

Classical linear regression (ordinary least squares, OLS) fits a line by minimizing the sum of squared residuals, which targets the conditional mean of the response variable. However, because OLS puts heavy weight on large deviations, its estimates can be strongly affected by even a few outliers. In contrast, quantile regression extends median regression to any conditional quantile (e.g. the 50% quantile) and minimizes an asymmetrically weighted absolute loss. A key advantage of quantile regression is its robustness: the influence function of the median is bounded, so regression quantiles inherit a high breakdown point. In practical terms, estimates like the conditional median are not pulled as far by extreme values. Prior studies confirm these properties: for example, Herawati (2020) showed in simulations that median regression provided smaller mean-squared error than OLS when outliers are present. In this article, we compare OLS and quantile regression for a linear model under controlled outlier contamination. 

# Linear Quantile Regression: Theory and Methods

## Definition and Estimation

For a random variable $Y$, the $\tau$-th quantile is the value $q_\tau$ such that $P(Y \le q_\tau) = \tau$. In a regression setting, **quantile regression (QR)** estimates:

$$
Q_Y(\tau \mid X = x) = x^\top \beta(\tau),
$$

where $\beta(\tau)$ is a vector of coefficients specific to quantile level $\tau$. For example, $\beta_1(0.5)$ represents the effect of $X_1$ on the median of $Y$.

Koenker and Bassett (1978) proposed estimating $\beta(\tau)$ by minimizing the **check loss**:

$$
\hat\beta(\tau) = \arg\min_\beta \sum_{i=1}^n \rho_\tau(y_i - x_i^\top \beta),
$$

with $\rho_\tau(u) = u(\tau - \mathbb{I}\{u < 0\})$. For $\tau = 0.5$, this reduces to **least absolute deviations (LAD)** regression.

Each $\tau$ is estimated independently using linear programming, and the family $\{\beta(\tau)\}$ forms a **quantile process** describing the full conditional distribution of $Y$.

## Inference

Under regularity conditions, $\hat\beta(\tau)$ is asymptotically normal:

$$
\sqrt{n}(\hat\beta(\tau) - \beta(\tau)) \overset{d}{\to} N(0, \Sigma(\tau)),
$$

with variance estimators obtained via **bootstrapping** or **sandwich estimators** (Koenker, 2005). Practical inference is available via `summary()` in the R package `quantreg`.

## Comparison to OLS

OLS estimates the conditional mean:

$$
\mathbb{E}[Y \mid X = x] = x^\top \beta,
$$

whereas QR estimates conditional quantiles. When errors are symmetric and homoscedastic, QR and OLS give similar results. Otherwise, QR captures **distributional heterogeneity**, such as increasing variance or skewness.

Moreover, QR is **robust to outliers in $Y$**, unlike OLS which minimizes squared error and is sensitive to extreme values. QR also allows different slopes across quantiles, offering richer interpretation.

In the next section, we illustrate these theoretical advantages through a simulation study.


# 3. Simulation Study

This section presents a simulation study designed to compare the performance of **Ordinary Least Squares (OLS)** and **Quantile Regression (QR)** estimators under controlled, interpretable scenarios. Our aim is to assess how both methods behave, particularly under outlier contamination.

## 3.1 Data Generating Process (DGP)

We consider the simple linear model \( Y = \beta_0 + \beta_1 X_1 + \varepsilon \), with fixed parameters \(\beta_0 = 5\) and \(\beta_1 = -1.5\). These values are chosen to induce a moderate negative slope and an interpretable intercept, ensuring that both OLS and QR coefficients remain in a tractable range for interpretation and graphical analysis. The predictor \(X_1\) is generated from a uniform distribution on \([0,10]\), which provides a constant density across its support and avoids introducing implicit bias or skewness into the covariate structure. The sample size \(n = 1000\) is selected to approximate asymptotic behavior while remaining computationally feasible. While we compute QR estimates for multiple quantiles (\(\tau \in \{0.1, 0.5, 0.9\}\)), the primary focus is on \(\tau = 0.5\), which corresponds to the conditional median and allows direct comparison with the OLS estimator of the conditional mean. As highlighted in Section 2, QR estimates \(\beta(\tau)\) independently for each \(\tau\), thus providing a richer description of the conditional distribution of \(Y\) than OLS.


```{r}
set.seed(666)
beta0 <- 5
beta1 <- 1.5 
n <- 1000
X1 <- runif(n, min = 0, max = 10)
tau=c(0.05,0.25,0.5,0.75,0.95)
```


## 3.2 Simulation Setup

To evaluate the estimators under realistic and adversarial conditions, we simulate three distinct error structures for \(\varepsilon\). First, we consider **homoscedastic Gaussian errors**: \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\) with \(\sigma = 2\), which fulfill all Gauss-Markov conditions and provide a benchmark for both estimators. Second, we introduce **contaminated Gaussian errors** to test robustness: 5% of the residuals are perturbed by a fixed additive shift (\(+50\)), applied to observations with the largest values of \(X_1\), thereby combining vertical outliers with high-leverage covariate values. Finally, we simulate **heteroskedastic errors**, where \(\varepsilon_i \sim \mathcal{N}(0, (1 + 0.2 X_{1i})^2)\). This violates the homoscedasticity assumption and allows us to observe how QR adapts across quantiles when the conditional variance of \(Y\) increases with \(X_1\).
  
```{r, echo= FALSE}
y_clean <- beta0 - beta1*X1 + rnorm(n, sd=2)


ols_clean <- lm(y_clean ~ X1)
qr_clean <- rq(y_clean ~ X1, tau =0.5)

```

```{r, echo= FALSE}
y_outliers <- y_clean
num_outliers <- round(0.05 * n)
out_idx <- order(X1, decreasing = TRUE)[1:num_outliers]
y_outliers[out_idx] <- y_outliers[out_idx] + 50
```


## 3.3 Evaluation Metrics

To quantify the behavior of the estimators, we combine visual inspection with numerical performance metrics. Given that QR minimizes absolute deviations, particularly for \(\tau = 0.5\), we employ the **Mean Absolute Error (MAE)** as the primary metric. MAE is defined as \(\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\), and aligns directly with the objective function of the LAD estimator. It provides a robust measure of predictive accuracy and is less sensitive to outliers than the **RMSE**, which disproportionately penalizes large deviations. This makes MAE more appropriate when comparing methods under contamination or heavy-tailed noise, as discussed by Koenker (2005). 


*****PENDING*****
The theoretical basis for QR (Section 2) suggests that:
- OLS is optimal under homoscedastic Gaussian errors (Gauss-Markov assumptions),
- QR provides robust, distribution-sensitive estimation that remains valid when these assumptions are violated.

This simulation seeks to verify those expectations empirically.


## 3.4 Simulation

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

```{r, fig.cap="OLS vs Multiple Quantile Regressions (Clean Data)", echo=FALSE}
plot(X1, y_clean,
     xlab = "X1", ylab = "Y", pch = 16, cex = 0.6, col = "grey")
abline(qr_clean, col = "blue", lwd = 1.5, lty = 1)
abline(ols_clean, col = "red", lwd = 3, lty = 2)

legend("topleft",
       legend = c(paste("QR tau =", 0.5), "OLS"),
       col = c(rep("blue"), "red"),
       lty = c(rep(1, length(tau)), 2),
       lwd = c(rep(1.5, length(tau)), 3),
       cex = 0.8)
```

```{r, echo=FALSE}
ols_coef <- coef(ols_clean)
qr_coef_matrix <- coef(qr_clean)


coef_df <- data.frame(OLS = ols_coef)

coef_df <- cbind(coef_df, qr_coef_matrix)

colnames(coef_df) <- c("OLS", paste("QR", sprintf("%.2f", 0.5)))

coef_df <- rownames_to_column(coef_df, var = "Coefficient")


kable_table <- knitr::kable(
  coef_df,
  digits = 3, # Round to 3 decimal places
  format = "pipe", # Use 'pipe' for markdown-like table, or 'html', 'latex'
  caption = "Comparison of OLS and Quantile Regression Coefficients (Clean Data) - Manual",
  align = c('l', rep('r', ncol(coef_df)-1)) # Align columns
)

kable_table
```

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


```{r, echo=FALSE}
ols_outliers = lm(y_outliers ~ X1)
qr_outliers = rq(y_outliers ~ X1, tau = 0.5)

ols_coef <- coef(ols_outliers)
qr_coef_matrix <- coef(qr_outliers)

coef_df <- data.frame(OLS = ols_coef)

coef_df <- cbind(coef_df, qr_coef_matrix)

colnames(coef_df) <- c("OLS", paste("QR", sprintf("%.2f", 0.5)))

coef_df <- rownames_to_column(coef_df, var = "Coefficient")


kable_table <- knitr::kable(
  coef_df,
  digits = 3, # Round to 3 decimal places
  format = "pipe", # Use 'pipe' for markdown-like table, or 'html', 'latex'
  caption = "Comparison of OLS and Quantile Regression Coefficients (Clean Data) - Manual",
  align = c('l', rep('r', ncol(coef_df)-1)) # Align columns
)

kable_table
```

```{r, fig.cap="OLS vs Quantile Regressions (Outliers Data)", echo=FALSE}
plot(X1, y_outliers,
     xlab = "X1", ylab = "Y", pch = 16, cex = 0.6, col = "grey")


qr_i <- rq(y_outliers ~ X1, tau = 0.5)
abline(qr_i, col = "blue", lwd = 1.5, lty = 1)


abline(ols_outliers, col = "red", lwd = 3, lty = 2)

legend("topleft",
       legend = c(paste("QR tau =", 0.5), "OLS"),
       col = c(rep("blue", length(0.5)), "red"),
       lty = c(rep(1, length(0.5)), 2),
       lwd = c(rep(1.5, length(0.5)), 3),
       cex = 0.8)
```

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

```{r plot-coef-vs-quantile, fig.cap="Slope vs Quantile for heteroskedastic case", echo=FALSE}
set.seed(666)
data_heterosk <- simulate_data(n=1000, heterosk=TRUE)
fits_heterosk <- lapply(seq(0.05, 0.95, 0.05), function(tau) rq(y ~ x1, tau = tau, data = data_heterosk))
slopes_heterosk <- sapply(fits_heterosk, function(fit) coef(fit)[2])
ols_slope <- coef(lm(y ~ x1, data = data_heterosk))[2]

plot_df <- tibble(
  tau = seq(0.05, 0.95, 0.05),
  slope = slopes_heterosk
)

ggplot(plot_df, aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  geom_hline(yintercept = ols_slope, color = "red", linetype = "dashed") +
  labs(x = "Quantile (tau)", y = "Slope", title = "Slope vs Quantile Level") +
  theme_minimal()
```

```{r anova-quantiles, eval=FALSE, echo=FALSE}
# fit25 <- rq(y ~ x1, tau=0.25, data=scenarios$clean_multi)
# fit50 <- rq(y ~ x1, tau=0.5, data=scenarios$clean_multi)
# fit75 <- rq(y ~ x1, tau=0.75, data=scenarios$clean_multi)
# anova_clean <- anova(fit25, fit50, fit75)
# 
# fit25_h <- rq(y ~ x1, tau=0.25, data=scenarios$outlier_multi)
# fit50_h <- rq(y ~ x1, tau=0.5, data=scenarios$outlier_multi)
# fit75_h <- rq(y ~ x1, tau=0.75, data=scenarios$outlier_multi)
# anova_outier <- anova(fit25_h, fit50_h, fit75_h)
# 
# # # Tables
# # kbl(anova_clean, caption = "ANOVA Test for Equality of Slopes across Quantiles (Clean Data)") %>%
# #   kable_styling(latex_options = c("hold_position", "striped"))
# # 
# # kbl(anova_outliers, caption = "ANOVA Test for Equality of Slopes across Quantiles (Outlier Data)") %>%
# #   kable_styling(latex_options = c("hold_position", "striped"))
```

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

```{r other-plots, fig.cap="Various plots",  echo=FALSE}
library(quantreg)
library(gridExtra)
library(dplyr)

# Simulate basic heteroskedastic data
set.seed(666)
n <- 500
X <- runif(n, 0, 10)
epsilon <- rnorm(n, mean = 0, sd = 1 + 0.3 * X)
Y <- 5 + 1.5 * X + epsilon
data <- data.frame(X = X, Y = Y)

# Fit models
ols_fit <- lm(Y ~ X, data = data)
rq_10 <- rq(Y ~ X, tau = 0.1, data = data)
rq_50 <- rq(Y ~ X, tau = 0.5, data = data)
rq_90 <- rq(Y ~ X, tau = 0.9, data = data)

# Plot 1: Scatterplot + regression lines
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_quantile(quantiles = c(0.1, 0.5, 0.9), color = "blue", size = 0.8) +
  labs(title = "OLS vs Quantile Regression Lines", y = "Y", x = "X") +
  theme_minimal()

# Plot 2: Slope across quantiles
taus <- seq(0.05, 0.95, by = 0.05)
slopes <- sapply(taus, function(tau) coef(rq(Y ~ X, tau = tau))[2])
p2 <- ggplot(data.frame(tau = taus, slope = slopes), aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = coef(ols_fit)[2], linetype = "dashed", color = "red") +
  labs(title = "Slope estimates across quantiles", y = "Slope", x = "Quantile (tau)") +
  theme_minimal()

# Plot 3: Robustness to outliers
data_outlier <- data
idx <- sample(1:n, 10)
data_outlier$Y[idx] <- data_outlier$Y[idx] + 50  # Add strong outliers

ols_fit_outlier <- lm(Y ~ X, data = data_outlier)
rq_50_outlier <- rq(Y ~ X, tau = 0.5, data = data_outlier)

p3 <- ggplot(data_outlier, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_abline(intercept = coef(rq_50_outlier)[1], slope = coef(rq_50_outlier)[2], color = "blue") +
  labs(title = "Effect of Outliers: OLS vs Median QR", y = "Y", x = "X") +
  theme_minimal()

# Plot 4: Residuals histograms
residuals_ols <- resid(ols_fit)
residuals_rq50 <- resid(rq_50)

residuals_df <- data.frame(
  residuals = c(residuals_ols, residuals_rq50),
  Method = rep(c("OLS", "QR (tau=0.5)"), each = n)
)

p4 <- ggplot(residuals_df, aes(x = residuals, fill = Method)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap(~Method) +
  labs(title = "Residuals Distribution: OLS vs QR", x = "Residuals", y = "Count") +
  theme_minimal()

# Arrange in grid (2x2 layout)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```


# Discussion & Conclusions. 

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

<!-- ***Extras and considerations***: This report has been made with a template in R Markdown, following the guidelines from [@Marcon2025], and the esource for the creation of reports in R Markdown, the bookdown package [@Xie2016]. -->
