library(quantreg)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(kableExtra)
library(tibble)

set.seed(666)
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)
set.seed(666)

#------ Simulation Functions -------------------------------------------------#
###############################################################################
# Core functions for simulation, model fitting, and outlier injection
beta0 <- 5; beta1 <- 1.5; beta2 <- -1.0
n_sim <- 100
outlier_frac <- 0.02
outlier_shift <- 30
tau_levels <- c(0.025, 0.25, 0.5, 0.75, 0.975)

# Core functions for simulation, model fitting, and outlier injection
beta0 <- 5; beta1 <- 1.5; beta2 <- -1.0
n_sim <- 100
outlier_frac <- 0.02
outlier_shift <- 30
tau_levels <- c(0.025, 0.25, 0.5, 0.75, 0.975)

simulate_data <- function(n = 200, multivariate = FALSE, heterosk = FALSE) {
  x1 <- runif(n, 0, 10)
  x2 <- if (multivariate) runif(n, 0, 10) else rep(0, n)
  sd_e <- if (heterosk) (1 + 0.6 * x1) else 2
  e <- rnorm(n, mean = 0, sd = sd_e)
  y <- beta0 + beta1 * x1 + beta2 * x2 + e
  data.frame(y = y, x1 = x1, x2 = x2)
}

add_high_leverage_point <- function(data) {
  new_x1 <- max(data$x1)
  new_x2 <- mean(data$x2) + 1
  new_y <- max(data$y) * 20
  new_point <- data.frame(y = new_y, x1 = new_x1, x2 = new_x2)
  rbind(data, new_point)
}

fit_models <- function(data, multivariate = FALSE) {
  if (multivariate) {
    ols <- lm(y ~ x1 + x2, data = data)
    qrs <- lapply(tau_levels, function(tau) rq(y ~ x1 + x2, tau = tau, data = data))
  } else {
    ols <- lm(y ~ x1, data = data)
    qrs <- lapply(tau_levels, function(tau) rq(y ~ x1, tau = tau, data = data))
  }
  list(ols = ols, qrs = qrs)
}

###############################################################################
#--- 4. Scenarios and Model Estimation ---------------------------------------#
###############################################################################
# Generate clean and outlier-contaminated datasets and fit models
multivariate_data <- simulate_data(n = 500, multivariate = TRUE, heterosk = TRUE)
multivariate_data_outlier <- add_high_leverage_point(multivariate_data)

scenarios <- list(
  clean_multi = multivariate_data,
  outlier_multi = multivariate_data_outlier
)

models <- list(
  clean_multi = fit_models(scenarios$clean_multi, multivariate = TRUE),
  outlier_multi = fit_models(scenarios$outlier_multi, multivariate = TRUE)
)

###############################################################################
#--- 5. Univariate Simulation and Basic Analysis -----------------------------#
###############################################################################
set.seed(666)
n <- 1000
X1 <- runif(n, 0, 10)
y_clean <- beta0 - beta1 * X1 + rnorm(n, sd = 2)
y_outliers <- y_clean
num_outliers <- round(outlier_frac * n)
out_idx <- order(X1, decreasing = TRUE)[1:num_outliers]
y_outliers[out_idx] <- y_outliers[out_idx] + outlier_shift

ols_clean <- lm(y_clean ~ X1)
qr_clean <- rq(y_clean ~ X1, tau = 0.5)
ols_outliers <- lm(y_outliers ~ X1)
qr_outliers <- rq(y_outliers ~ X1, tau = 0.5)

###############################################################################
#--- 6. Plots and Tables -----------------------------------------------------#
###############################################################################
# Plot 1: Clean data OLS vs QR
plot(X1, y_clean, xlab = "X1", ylab = "Y", pch = 16, cex = 0.6, col = "grey")
abline(qr_clean, col = "blue", lwd = 1.5)
abline(ols_clean, col = "red", lwd = 3, lty = 2)
legend("topleft", legend = c("QR tau = 0.5", "OLS"), col = c("blue", "red"), lty = c(1, 2))

# Plot 2: Outliers impact on OLS vs QR
plot(X1, y_outliers, xlab = "X1", ylab = "Y", pch = 16, cex = 0.6, col = "grey")
abline(qr_outliers, col = "blue", lwd = 1.5)
abline(ols_outliers, col = "red", lwd = 3, lty = 2)
legend("topleft", legend = c("QR tau = 0.5", "OLS"), col = c("blue", "red"), lty = c(1, 2))

# Table: Coefficient comparison
coef_table <- tibble(
  Coefficient = names(coef(ols_clean)),
  OLS_Clean = coef(ols_clean),
  QR_Clean = coef(qr_clean),
  OLS_Outliers = coef(ols_outliers),
  QR_Outliers = coef(qr_outliers)
)
print(knitr::kable(coef_table, digits = 3, caption = "OLS vs QR Coefficient Comparison"))

# Plot 3: Slope vs Quantile for heteroskedastic data
data_heterosk <- simulate_data(n = 1000, heterosk = TRUE)
fits_heterosk <- lapply(seq(0.05, 0.95, 0.05), function(tau) rq(y ~ x1, tau = tau, data = data_heterosk))
slopes_heterosk <- sapply(fits_heterosk, function(fit) coef(fit)[2])
ols_slope <- coef(lm(y ~ x1, data = data_heterosk))[2]

plot_df <- tibble(tau = seq(0.05, 0.95, 0.05), slope = slopes_heterosk)

print(
  ggplot(plot_df, aes(x = tau, y = slope)) +
    geom_line(color = "blue") +
    geom_point(color = "blue") +
    geom_hline(yintercept = ols_slope, color = "red", linetype = "dashed") +
    labs(x = "Quantile (tau)", y = "Slope", title = "Slope vs Quantile Level") +
    theme_minimal()
)

###############################################################################
#--- 7. Optional: Grid Summary Plots ----------------------------------------#
###############################################################################
# Summary of OLS vs QR under heteroskedasticity with and without outliers
n <- 500
X <- runif(n, 0, 10)
epsilon <- rnorm(n, mean = 0, sd = 1 + 0.3 * X)
Y <- 5 + 1.5 * X + epsilon
data <- data.frame(X = X, Y = Y)

ols_fit <- lm(Y ~ X, data = data)
rq_10 <- rq(Y ~ X, tau = 0.1, data = data)
rq_50 <- rq(Y ~ X, tau = 0.5, data = data)
rq_90 <- rq(Y ~ X, tau = 0.9, data = data)

# Plot 1
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_quantile(quantiles = c(0.1, 0.5, 0.9), color = "blue", size = 0.8) +
  labs(title = "OLS vs Quantile Regression Lines", y = "Y", x = "X") +
  theme_minimal()

# Plot 2
taus <- seq(0.05, 0.95, by = 0.05)
slopes <- sapply(taus, function(tau) coef(rq(Y ~ X, tau = tau))[2])
p2 <- ggplot(data.frame(tau = taus, slope = slopes), aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = coef(ols_fit)[2], linetype = "dashed", color = "red") +
  labs(title = "Slope estimates across quantiles", y = "Slope", x = "Quantile (tau)") +
  theme_minimal()

# Plot 3
idx <- sample(1:n, 10)
data_outlier <- data
data_outlier$Y[idx] <- data_outlier$Y[idx] + 50
ols_fit_outlier <- lm(Y ~ X, data = data_outlier)
rq_50_outlier <- rq(Y ~ X, tau = 0.5, data = data_outlier)

p3 <- ggplot(data_outlier, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_abline(intercept = coef(rq_50_outlier)[1], slope = coef(rq_50_outlier)[2], color = "blue") +
  labs(title = "Effect of Outliers: OLS vs Median QR", y = "Y", x = "X") +
  theme_minimal()

# Plot 4
residuals_ols <- resid(ols_fit)
residuals_rq50 <- resid(rq_50)
residuals_df <- data.frame(
  residuals = c(residuals_ols, residuals_rq50),
  Method = rep(c("OLS", "QR (tau=0.5)"), each = n)
)
p4 <- ggplot(residuals_df, aes(x = residuals, fill = Method)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap(~Method) +
  labs(title = "Residuals Distribution: OLS vs QR", x = "Residuals", y = "Count") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, nrow = 2)



#Section 3: 
## 3. Simulation Study

This section presents a simulation study designed to compare the performance of **Ordinary Least Squares (OLS)** and **Quantile Regression (QR)** estimators under controlled, interpretable scenarios. Our aim is to assess how both methods behave, particularly under outlier contamination.

### 3.1 Data Generating Process (DGP)

We consider the simple linear model \( Y = \beta_0 + \beta_1 X_1 + \varepsilon \), with fixed parameters \(\beta_0 = 5\) and \(\beta_1 = -1.5\). These values are chosen to induce a moderate negative slope and an interpretable intercept, ensuring that both OLS and QR coefficients remain in a tractable range for interpretation and graphical analysis. The predictor \(X_1\) is generated from a uniform distribution on \([0,10]\), which provides a constant density across its support and avoids introducing implicit bias or skewness into the covariate structure. The sample size \(n = 1000\) is selected to approximate asymptotic behavior while remaining computationally feasible. While we compute QR estimates for multiple quantiles (\(\tau \in \{0.1, 0.5, 0.9\}\)), the primary focus is on \(\tau = 0.5\), which corresponds to the conditional median and allows direct comparison with the OLS estimator of the conditional mean. As highlighted in Section 2, QR estimates \(\beta(\tau)\) independently for each \(\tau\), thus providing a richer description of the conditional distribution of \(Y\) than OLS.

### 3.2 Simulation Setup

To evaluate the estimators under realistic and adversarial conditions, we simulate three distinct error structures for \(\varepsilon\). First, we consider **homoscedastic Gaussian errors**: \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\) with \(\sigma = 2\), which fulfill all Gauss-Markov conditions and provide a benchmark for both estimators. Second, we introduce **contaminated Gaussian errors** to test robustness: 5% of the residuals are perturbed by a fixed additive shift (\(+50\)), applied to observations with the largest values of \(X_1\), thereby combining vertical outliers with high-leverage covariate values. Finally, we simulate **heteroskedastic errors**, where \(\varepsilon_i \sim \mathcal{N}(0, (1 + 0.2 X_{1i})^2)\). This violates the homoscedasticity assumption and allows us to observe how QR adapts across quantiles when the conditional variance of \(Y\) increases with \(X_1\).

### 3.3 Evaluation Metrics

To quantify the behavior of the estimators, we combine visual inspection with numerical performance metrics. Given that QR minimizes absolute deviations, particularly for \(\tau = 0.5\), we employ the **Mean Absolute Error (MAE)** as the primary metric. MAE is defined as \(\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\), and aligns directly with the objective function of the LAD estimator. It provides a robust measure of predictive accuracy and is less sensitive to outliers than the **RMSE**, which disproportionately penalizes large deviations. This makes MAE more appropriate when comparing methods under contamination or heavy-tailed noise, as discussed by Koenker (2005). 



###############################################################################
#--- Quantile Regression Sensitivity to Outliers Under Different Errors ---#
###############################################################################

# QR estimation and MAE evaluation function
evaluate_qr <- function(y_obs, y_true, X1, label) {
  fit <- rq(y_obs ~ X1, tau = 0.5)
  pred <- predict(fit)
  mae <- mean(abs(y_true - pred))
  data.frame(
    Model = label,
    Intercept = coef(fit)[1],
    Slope = coef(fit)[2],
    MAE = mae
  )
}

# Gaussian (homoscedastic)
e_gauss <- rnorm(n, 0, 2)
y_gauss <- beta0 - beta1 * X1 + e_gauss
eval_gauss <- evaluate_qr(y_obs = y_gauss, y_true = y_gauss, X1 = X1, label = "QR - Gaussian")

# Contaminated
y_contam <- y_gauss
contam_idx <- order(X1, decreasing = TRUE)[1:round(0.05 * n)]
y_contam[contam_idx] <- y_contam[contam_idx] + 50
eval_contam <- evaluate_qr(y_obs = y_contam, y_true = y_gauss, X1 = X1, label = "QR - Contaminated")

# Heteroskedastic
sd_hetero <- 1 + 0.2 * X1
e_hetero <- rnorm(n, 0, sd = sd_hetero)
y_hetero <- beta0 - beta1 * X1 + e_hetero
eval_hetero <- evaluate_qr(y_obs = y_hetero, y_true = y_hetero, X1 = X1, label = "QR - Heteroskedastic")

# Combine results
qr_eval_all <- bind_rows(eval_gauss, eval_contam, eval_hetero)
print(knitr::kable(qr_eval_all, digits = 3, caption = "QR Estimates and MAE under Different Error Structures"))


# Plots
par(mfrow = c(1, 3))
plot(X1, y_gauss, main = "QR - Gaussian", xlab = "X1", ylab = "Y", pch = 16, col = rgb(0, 0, 0, 0.3))
abline(rq(y_gauss ~ X1, tau = 0.5), col = "blue", lwd = 2)

plot(X1, y_contam, main = "QR - Contaminated", xlab = "X1", ylab = "Y", pch = 16, col = rgb(0, 0, 0, 0.3))
abline(rq(y_contam ~ X1, tau = 0.5), col = "blue", lwd = 2)

plot(X1, y_hetero, main = "QR - Heteroskedastic", xlab = "X1", ylab = "Y", pch = 16, col = rgb(0, 0, 0, 0.3))
abline(rq(y_hetero ~ X1, tau = 0.5), col = "blue", lwd = 2)

