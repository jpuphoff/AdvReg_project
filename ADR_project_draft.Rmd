---
title: "Advance Regression Term Project"
author:
  - name: "Gerard Palomo & Juan Pablo Uphoff"
    affiliation:
      - affsuperscript: 1
        dptuniv: "Department of Statistics / University Carlos III of Madrid"
        address: >
          Calle Madrid 126 Getafe,
          28903,
          Spain.
corrauthor: 
  url: XXXXXXXXX@alumnos.uc3m.es and 100508278@alumnos.uc3m.es
abstract: >
    Linear quantile regression (QR) extends ordinary least squares (OLS) by modeling conditional quantiles, offering a richer view of the response variable's distribution beyond the conditional mean provided by OLS. This paper highlights two key advantages of QR over OLS. Firstly, QR provides robustness to outliers in the response variable, a significant limitation for OLS which relies on minimizing squared errors. Secondly, QR allows for modeling distributional heterogeneity, such as heteroscedasticity, which OLS inherently overlooks by focusing solely on the mean. We compare the performance of OLS and linear QR estimators through a simulation study in a univariate setting. We examine performance under three conditions: (1) a baseline scenario with homoscedastic Gaussian errors, (2) contamination where outliers are introduced at high-leverage points to assess robustness, and (3) heteroscedastic errors to assess distributional modeling capabilities. We specifically compare OLS with QR at the 0.1, 0.5 (median), and 0.9 quantiles. Key performance metrics, including Mean Absolute Error (MAE) and coefficient stability, are assessed. Results demonstrate that QR estimates, particularly the median ($\tau=0.5$), remain reliable under outlier contamination where OLS estimates become significantly biased. Furthermore, QR effectively captures distributional effects like heteroscedasticity, providing quantile-specific insights that OLS cannot. All simulation code is provided in R for reproducibility.
keywords: [Quantile Regression, Robustness, Heteroscedasticity, OLS, Outliers]
JEL: [C15, C21]
acknowledgements: >
  This report was completed for the course **Advanced Regression and Prediction**, as part of the **MSc in Statistics for Data Science** at **University Carlos III of Madrid**.
#journalinfo: "Working Paper, MSc in Statistics for Data Science, Carlos III University of Madrid"
#archive: "DOI: N/A (internal report)"
date: "`r Sys.Date()`"
lang: en-US
keywordlabel: Keywords
JELlabel: JEL
acknowledgementslabel: Acknowledgements
corrauthorlabel: Contact
bibliography: references.bib
biblio-style: apalike
toc-depth: 3
fontsize: 10pt
urlcolor: blue
preamble: >
  \hyphenation{quan-tile re-gres-sion out-li-ers het-ero-ske-das-tic}
always_allow_html: yes
csquotes: true
output:
  bookdown::pdf_book:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: yes
    toc: false
    number_sections: true
    fig_caption: true
    highlight: tango
    geometry: margin=1in
    linestretch: 1.5
    mainfont: "Times New Roman"  
    monofont: "Courier New"  
    template: latex/template.tex
---

```{r}
#| label: DontModify
#| include: false
### Utilities. Do not modify.
install_packages <- function(pkgs) {
  installed <- rownames(installed.packages())
  to_install <- setdiff(pkgs, installed)
  if (length(to_install) > 0) install.packages(to_install)
}
install_packages(c("bookdown", "tidyverse", "quantreg", "kableExtra", "gridExtra"))

library(quantreg)
library(gridExtra)
library(kableExtra)
library(tidyverse)
```

```{r}
#| label: Options
#| include: false

# knitr options
knitr::opts_chunk$set(
  cache = TRUE, # Cache chunk results
  echo = FALSE, # Show/Hide code
  warning = FALSE, # Show/Hide warnings
  message = FALSE, # Show/Hide messages
  # Figure alignment and size
  fig.align = "center", out.width = "80%",
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
)
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)
knitr::opts_chunk$set(dev.args = list(bg = "transparent"))
```

```{r}
#| label: set up & simulation functions
#| include: false
#| echo: false

library(quantreg)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(kableExtra)
library(tibble)

set.seed(666)
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)

# Core functions for simulation, model fitting, and outlier injection
beta0 <- 5; beta1 <- 1.5
n_sim <- 100
outlier_frac <- 0.02
outlier_shift <- 50
tau_levels <- c(0.1, 0.5, 0.9)

simulate_data <- function(n = 200, heterosk = FALSE) {
  x1 <- runif(n, 0, 10)
  sd_e <- if (heterosk) (1 + 0.6 * x1) else 2
  e <- rnorm(n, mean = 0, sd = sd_e)
  y <- beta0 + beta1 * x1 + e
  data.frame(y = y, x1 = x1)
}

fit_models <- function(data) {
  ols <- lm(y ~ x1, data = data)
  qrs <- lapply(tau_levels, function(tau) rq(y ~ x1, tau = tau, data = data))
  list(ols = ols, qrs = qrs)
}

add_high_leverage_point <- function(data) {
  new_x1 <- max(data$x1) + 1
  new_y <- max(data$y) * 20
  new_point <- data.frame(y = new_y, x1 = new_x1)
  rbind(data, new_point)
}

```

# Introduction

Classical linear regression, estimated via ordinary least squares (OLS), focuses on modeling the conditional mean of a response variable by minimizing the sum of squared residuals. While powerful under ideal assumptions, OLS faces significant limitations in practice. Its reliance on squared errors renders estimates highly sensitive to outliers, potentially leading to biased results. Furthermore, OLS provides only a partial view of the conditional distribution by focusing solely on the central tendency and typically assumes homoscedastic errors, limiting its ability to describe relationships where the variability of the response changes with predictors.

Quantile regression (QR), introduced by \citet{Koenker1978}, offers a more comprehensive and robust alternative. By modeling conditional quantiles (e.g., the median, quartiles, deciles), QR addresses the shortcomings of OLS in two crucial ways. First, its estimation, based on minimizing an asymmetrically weighted sum of absolute errors (the check loss function), provides inherent robustness against outliers in the response variable; the influence of extreme observations is bounded, unlike in OLS. Prior studies, such as \citet{Onyedikachi2015}, have confirmed the superior performance of quantile regression over OLS in the presence of outliers. Second, QR provides a mechanism to characterize the entire conditional distribution of the response variable, not just its mean. This allows researchers to understand how predictors affect different parts of the distribution, making it particularly well-suited for analyzing data with heteroscedasticity or other forms of distributional heterogeneity.

This report aims to compare the performance of OLS and linear quantile regression estimators, focusing on these two key advantages of QR. We will demonstrate the robustness advantage of QR, particularly the conditional median ($\tau=0.5$), under controlled outlier contamination in a simple linear model, contrasting it with the sensitivity of OLS. Additionally, we will illustrate QR's ability to capture distributional heterogeneity by examining its performance under heteroscedastic errors, showcasing how it provides insights beyond the conditional mean estimated by OLS. We conduct a simulation study designed to highlight these differences visually and quantitatively, examining estimator behavior under three distinct scenarios: (1) a baseline homoscedastic Gaussian setting, (2) the same setting contaminated with high-leverage outliers, and (3) a setting with heteroscedastic errors. Coefficient stability and prediction accuracy (using Mean Absolute Error) are assessed across these scenarios.

# Linear Quantile Regression: Theory and Methods

## Definition and Estimation

For a random variable $Y$, the $\tau$-th quantile is the value $q_\tau$ such that $P(Y \le q_\tau) = \tau$. In a regression setting, quantile regression (QR) estimates:

$$
Q_Y(\tau \mid X = x) = x^\top \beta(\tau),
$$

where $\beta(\tau)$ is a vector of coefficients specific to quantile level $\tau$. For example, $\beta_1(0.5)$ represents the effect of $X_1$ on the median of $Y$.

\citet{Koenker1978} proposed estimating $\beta(\tau)$ by minimizing the check loss:

$$
\hat\beta(\tau) = \arg\min_\beta \sum_{i=1}^n \rho_\tau(y_i - x_i^\top \beta),
$$

with $\rho_\tau(u) = u(\tau - \mathbb{I}\{u < 0\})$. For $\tau = 0.5$, this reduces to least absolute deviations (LAD) regression.

Each $\tau$ is estimated independently using linear programming, and the family $\{\beta(\tau)\}$ forms a quantile process describing the full conditional distribution of $Y$.

## Inference

Under regularity conditions, $\hat\beta(\tau)$ is asymptotically normal:

$$
\sqrt{n}(\hat\beta(\tau) - \beta(\tau)) \overset{d}{\to} N(0, \Sigma(\tau)),
$$

with variance estimators obtained via bootstrapping or sandwich estimators \citep{Koenker2005}.

## Comparison to OLS

OLS estimates the conditional mean:

$$
\mathbb{E}[Y \mid X = x] = x^\top \beta,
$$

whereas QR estimates conditional quantiles. When errors are symmetric and homoscedastic, QR and OLS give similar results. Otherwise, QR captures distributional heterogeneity, such as increasing variance or skewness.

Moreover, QR is robust to outliers in $Y$, unlike OLS which minimizes squared error and is sensitive to extreme values. QR also allows different slopes across quantiles, offering richer interpretation.

In the next section, we illustrate these theoretical advantages through a simulation study.


# Simulation Study

This section presents a simulation study designed to compare the performance of **Ordinary Least Squares (OLS)** and **Quantile Regression (QR)** estimators under controlled, interpretable scenarios. Our aim is to assess how both methods behave, particularly under outlier contamination and additionally under heteroscedasticity.

To focus on the theoretical properties discussed in Section 2, we restrict attention to a simple univariate linear model with a single predictor. This allows for clean interpretation and visual representation of the results. Additionally, we consider three error structures: a baseline homoscedastic Gaussian case, a contaminated version with outliers at high-leverage points and a heteroscedastic case. This controlled setup isolates the impact of extreme observations on both methods, and is helps to reveal the key differences in robustness and sensitivity. 

## Data Generating Process (DGP)

We consider the simple linear model $Y = \beta_0 + \beta_1 X_1 + \varepsilon$, with fixed parameters $\beta_0 = 5$ and $\beta_1 = -1.5$. These values are chosen to induce a moderate negative slope and an interpretable intercept, ensuring that both OLS and QR coefficients remain in a tractable range for interpretation and graphical analysis. The predictor $X_1$ is generated from a uniform distribution on $[0,10]$, which provides a constant density across its support and avoids introducing implicit bias or skewness into the covariate structure. The sample size $n = 1000$ is selected to approximate asymptotic behavior while remaining computationally feasible. While we compute QR estimates for multiple quantiles ($\tau \in \{0.1, 0.5, 0.9\}$), the primary focus is on $\tau = 0.5$, which corresponds to the conditional median and allows direct comparison with the OLS estimator of the conditional mean. As highlighted in Section 2, QR estimates $\beta(\tau)$ independently for each $\tau$, thus providing a richer description of the conditional distribution of $Y$ than OLS.


## Simulation Setup

To evaluate the estimators under different conditions relevant to their theoretical properties, we simulate data using the DGP described above under three distinct error structures for $\varepsilon$:

1.  **Baseline Homoscedastic Gaussian Errors:** We first consider $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ with a constant variance $\sigma = 2$. This scenario satisfies the classical OLS assumptions and serves as a benchmark for comparing OLS and QR under ideal conditions.

2.  **Contaminated Gaussian Errors (Outlier Robustness Test):** To assess robustness, we start with the baseline homoscedastic Gaussian errors and introduce contamination. Specifically, 2% of the observations ($n \times 0.02$) with the largest values of the predictor $X_1$ (high-leverage points) have their corresponding error terms perturbed by a large positive constant ($+50$). This creates vertical outliers combined with leverage, designed to maximally challenge the estimators' stability.

3.  **Heteroscedastic Gaussian Errors (Distributional Modeling Test):** To illustrate the ability of QR to model distributional heterogeneity, we simulate errors whose variance increases linearly with the predictor: $\varepsilon_i \sim \mathcal{N}(0, \sigma_i^2)$, where $\sigma_i = \sigma_0 (1 + \gamma X_{1i})$. We use parameters like $\sigma_0 = 1$ and $\gamma = 0.3$ to generate noticeable heteroscedasticity. This scenario violates the OLS assumption of constant variance.

For each scenario, we fit both the OLS model and QR models at $\tau \in \{0.1, 0.5, 0.9\}$.

## Evaluation Metrics

To quantify the behavior of the estimators, we combine visual inspection with numerical performance metrics. Given that QR minimizes absolute deviations, particularly for $\tau = 0.5$, we employ the **Mean Absolute Error (MAE)** as the primary metric. MAE is defined as $\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$, and aligns directly with the objective function of the LAD estimator. It provides a robust measure of predictive accuracy and is less sensitive to outliers than the **RMSE**, which disproportionately penalizes large deviations. This makes MAE more appropriate when comparing methods under contamination or heavy-tailed noise, as discussed by \citet{Koenker2005}.


## Simulation

We now illustrate the theoretical differences between OLS and quantile regression through a basic univariate simulation. Using the DGP defined in Section 3.1, we compare both estimators under clean and contaminated conditions. We focus on the conditional median estimate (QR at $\tau = 0.5$) and examine how each method responds to the presence of vertical outliers in high-leverage positions.

```{r echo=FALSE}
set.seed(666)
n <- 1000
X1 <- runif(n, 0, 10)
y_clean <- beta0 - beta1 * X1 + rnorm(n, sd = 2)
y_outliers <- y_clean
num_outliers <- round(outlier_frac * n)
out_idx <- order(X1, decreasing = TRUE)[1:num_outliers]
y_outliers[out_idx] <- y_outliers[out_idx] + outlier_shift

ols_clean <- lm(y_clean ~ X1)
qr_clean <- rq(y_clean ~ X1, tau = 0.5)
ols_outliers <- lm(y_outliers ~ X1)
qr_outliers <- rq(y_outliers ~ X1, tau = 0.5)

```
Figure 1 shows that both OLS (dashed red line) and quantile regression (blue line) yield nearly identical slope estimates when applied to clean, homoscedastic data, in line with the theoretical results discussed in Section 2.3. However, under contamination, even with as little as 2% of vertical high-leverage outliers, the OLS slope is  distorted—flattening as it attempts to minimize squared error. In contrast, the QR line remains unaffected, producing a slope estimate that better reflects the central structure of the data. This illustrates the robustness property of quantile regression highlighted earlier: since QR minimizes a weighted absolute loss, it is less sensitive to large deviations in the response, and more resilient to local anomalies.

```{r fig.width=9, fig.height=6, fig.cap="Comparison of slope estimates across settings"}
# Plot 1: Clean data
df_clean <- data.frame(X1 = X1, Y = y_clean)
p_1 <- ggplot(df_clean, aes(x = X1, y = Y)) +
  geom_point(color = "grey") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linetype = "dashed") +
  geom_quantile(quantiles = 0.5, method = "rq", formula = y ~ x, color = "blue") +
  labs(title = "Clean Data: OLS vs QR (tau = 0.5)")

# Plot 2: With outliers
df_out <- data.frame(X1 = X1, Y = y_outliers)
p_2 <- ggplot(df_out, aes(x = X1, y = Y)) +
  geom_point(color = "grey") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linetype = "dashed") +
  geom_quantile(quantiles = 0.5, method = "rq", formula = y ~ x, color = "blue") +
  labs(title = "With Outliers: OLS vs QR (tau = 0.5)")

# Combine plots
grid.arrange(p_1, p_2, nrow = 1)
```

```{r include=FALSE}
# Table: Coefficient comparison
coef_table <- tibble(
  Coefficient = names(coef(ols_clean)),
  OLS_Clean = coef(ols_clean),
  QR_Clean = coef(qr_clean),
  OLS_Outliers = coef(ols_outliers),
  QR_Outliers = coef(qr_outliers)
)
knitr::kable(coef_table, digits = 3, format = "latex", booktabs = TRUE,
             caption = "OLS vs Quantile Regression Coefficient Comparison") %>%
  kable_styling(latex_options = c("hold_position"))
```

Figure 2 illustrates how the estimated slope coefficient in quantile regression varies across quantile levels $\tau \in (0.05, 0.95)$ in a heteroskedastic setting. The increasing slope as $\tau$ increases reflects the presence of conditional heteroskedasticity: higher quantiles are associated with greater dispersion in the response variable $Y$, which alters the marginal effect of $X_1$ across the conditional distribution. Quantile regression estimates $\beta_1(\tau)$ independently for each $\tau$, capturing variation in the conditional distribution of $Y$ while the slope of OLS remains constant because it targets the conditional mean and assumes homoscedasticity.

```{r fig.width=9, fig.height=6, fig.cap="Slope vs Quantile for heteroskedastic data"}
set.seed(666)
data_heterosk <- simulate_data(n = 1000, heterosk = TRUE)
fits_heterosk <- lapply(seq(0.05, 0.95, 0.05), function(tau) rq(y ~ x1, tau = tau, data = data_heterosk))
slopes_heterosk <- sapply(fits_heterosk, function(fit) coef(fit)[2])
ols_slope <- coef(lm(y ~ x1, data = data_heterosk))[2]

plot_df <- tibble(tau = seq(0.05, 0.95, 0.05), slope = slopes_heterosk)

print(
  ggplot(plot_df, aes(x = tau, y = slope)) +
    geom_line(color = "darkblue") +
    geom_point(color = "darkblue") +
    geom_hline(yintercept = ols_slope, color = "red", linetype = "dashed") +
    labs(x = "Quantile (tau)", y = "Slope", title = "Slope vs Quantile Level") +
    theme_minimal()
)
```

```{r echoFALSE}
set.seed(666)

evaluate_model <- function(y_obs, y_true, X1, label, method = "qr") {
  if (method == "qr") {
    fit <- rq(y_obs ~ X1, tau = 0.5)
  } else {
    fit <- lm(y_obs ~ X1)
  }
  pred <- predict(fit)
  mae <- mean(abs(y_true - pred))
  rmse <- sqrt(mean((y_true - pred)^2))
  slope_bias <- coef(fit)[2] + 1.5  # because true beta1 = -1.5
  data.frame(
    Model = label,
    Method = toupper(method),
    Intercept = coef(fit)[1],
    Slope = coef(fit)[2],
    Slope_Bias = slope_bias,
    MAE = mae,
    RMSE = rmse
  )
}

# Evaluations
evals <- bind_rows(
  evaluate_model(y_clean, y_clean, X1, "Gaussian", method = "qr"),
  evaluate_model(y_outliers, y_clean, X1, "Contaminated", method = "qr"),
  evaluate_model(y_clean, y_clean, X1, "Gaussian", method = "ols"),
  evaluate_model(y_outliers, y_clean, X1, "Contaminated", method = "ols")
)

knitr::kable(evals, digits = 3, format = "latex", booktabs = TRUE,
             caption = "OLS vs Quantile Regression: Coefficients and Error Metrics under Different Error Structures") %>%
  kable_styling(latex_options = c("hold_position"))
```

Figure 3 presents a multifaceted comparison between OLS and quantile regression under heteroskedasticity and contamination. In the top-left panel, QR lines at $\tau = 0.1, 0.5, 0.9$ capture the conditional distributional spread of $Y$, while OLS fits a single conditional mean. The top-right panel plots the estimated slope $\beta_1(\tau)$ across quantiles. The non-constant pattern confirms that the marginal effect of $X$ varies with $\tau$, violating the constant-slope assumption implicit in OLS, as discussed in Section 2.3. The bottom-left panel illustrates robustness: under contamination, OLS is pulled downward due to extreme values in $Y$, while the median QR line remains stable. Finally, the residual histograms (bottom-right) show that OLS residuals are more dispersed and asymmetric, reflecting its sensitivity to outliers, whereas QR residuals remain more concentrated. 

```{r fig.width=9, fig.height=6, fig.cap="Slope vs Quantile for heteroskedastic data"}
# Summary of OLS vs QR under heteroskedasticity with and without outliers
set.seed(666)
n <- 500
X <- runif(n, 0, 10)
epsilon <- rnorm(n, mean = 0, sd = 1 + 0.3 * X)
Y <- 5 + 1.5 * X + epsilon
data <- data.frame(X = X, Y = Y)

ols_fit <- lm(Y ~ X, data = data)
rq_10 <- rq(Y ~ X, tau = 0.1, data = data)
rq_50 <- rq(Y ~ X, tau = 0.5, data = data)
rq_90 <- rq(Y ~ X, tau = 0.9, data = data)

# Plot 1
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_quantile(quantiles = c(0.1, 0.5, 0.9), color = "blue", linewidth = 0.8) +
  labs(title = "OLS vs Quantile Regression Lines", y = "Y", x = "X") +
  theme_minimal()

# Plot 2
taus <- seq(0.05, 0.95, by = 0.05)
slopes <- sapply(taus, function(tau) coef(rq(Y ~ X, tau = tau))[2])
p2 <- ggplot(data.frame(tau = taus, slope = slopes), aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = coef(ols_fit)[2], linetype = "dashed", color = "red") +
  labs(title = "Slope estimates across quantiles", y = "Slope", x = "Quantile (tau)") +
  theme_minimal()

# Plot 3
idx <- sample(1:n, 10)
data_outlier <- data
data_outlier$Y[idx] <- data_outlier$Y[idx] + 50
ols_fit_outlier <- lm(Y ~ X, data = data_outlier)
rq_50_outlier <- rq(Y ~ X, tau = 0.5, data = data_outlier)

p3 <- ggplot(data_outlier, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_abline(intercept = coef(rq_50_outlier)[1], slope = coef(rq_50_outlier)[2], color = "blue") +
  labs(title = "Effect of Outliers: OLS vs Median QR", y = "Y", x = "X") +
  theme_minimal()

# Plot 4
residuals_ols <- resid(ols_fit)
residuals_rq50 <- resid(rq_50)
residuals_df <- data.frame(
  residuals = c(residuals_ols, residuals_rq50),
  Method = rep(c("OLS", "QR (tau=0.5)"), each = n)
)
p4 <- ggplot(residuals_df, aes(x = residuals, fill = Method)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap(~Method) +
  labs(title = "Residuals Distribution: OLS vs QR", x = "Residuals", y = "Count") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Table 2 illustrates the comparative robustness of OLS and quantile regression (QR at $\tau = 0.5$ under clean and contaminated settings. Both estimators perform similarly in the Gaussian case. However, under contamination, OLS coefficients exhibit severe bias, particularly in the slope, while QR remains stable. This behavior is theoretically consistent with QR's bounded influence function and its minimization of the check loss, which limits sensitivity to extreme values. MAE, aligned with the LAD objective, remains nearly unchanged for QR, whereas OLS shows substantial deterioration. RMSE, although not optimized by QR, is included for standard comparison and further confirms OLS's vulnerability under outlier contamination.

Figure 4 shows the evolution of the estimated slope $\hat{\beta}_1$ as a function of outlier contamination. QR remains stable under moderate contamination, exhibiting high resistance to leverage-induced distortion. However, once the proportion of outliers exceeds a critical threshold (around 25%), their influence becomes structurally dominant—shifting the conditional median itself and leading to breakdown. In contrast, OLS degrades continuously, with no resistance to contamination at any level.
```{r fig.width=9, fig.height=6, fig.cap="Estimated slope vs outlier contamination"}
# OLS vs. QR: effect of outliers
set.seed(666)
data_base <- simulate_data(n = n, heterosk = FALSE)
X1 <- data_base$x1
y_base <- data_base$y
contam_levels <- seq(0, 0.5, by = 0.02)  # 0% to 30% de outliers

estimate_slope <- function(frac, method) {
  y_mod <- y_base
  if (frac > 0) {
    idx <- order(X1, decreasing = TRUE)[1:round(frac * n)]
    y_mod[idx] <- y_mod[idx] + outlier_shift
  }
  fit <- if (method == "ols") lm(y_mod ~ X1) else rq(y_mod ~ X1, tau = 0.5)
  coef(fit)[2]
}

results <- expand.grid(Contamination = contam_levels, Method = c("OLS", "QR"))
results$Slope <- mapply(estimate_slope, results$Contamination, tolower(results$Method))

# Plot
ggplot(results, aes(x = Contamination, y = Slope, color = Method)) +
  geom_line(linewidth = 1.2) +
  geom_hline(yintercept = beta1, linetype = "dashed", color = "black") +
  labs(
    title = "Estimated Slope vs Outlier Contamination",
    x = "Contamination Fraction",
    y = expression(hat(beta)[1])
  ) +
  theme_minimal()
```

# Discussion & Conclusions

This report has compared ordinary least squares (OLS) and quantile regression (QR) for modeling linear relationships, focusing on scenarios where classical OLS assumptions are violated. Through controlled simulations, we demonstrated two primary advantages of QR. Firstly, QR, particularly median regression ($\tau=0.5$$, exhibits significant **robustness to outliers**, providing stable and reliable coefficient estimates even under contamination with high-leverage points, a condition where OLS estimates suffered severe bias (as shown in Figure 1 and Table 2. This resilience stems from QR's use of the check loss function, which minimizes absolute deviations rather than squared deviations.

Second, QR effectively models distributional heterogeneity, capturing how predictor effects vary across the conditional distribution. Our heteroscedasticity simulations (Figures 2 and 3) revealed trends and spread that OLS, focused solely on the conditional mean, entirely misses. In conclusion, while OLS remains a foundational method for estimating conditional means, quantile regression (QR) provides a more versatile framework—particularly in settings involving outliers, heteroscedasticity, or deviations from Gaussian error structures—by capturing the full conditional distribution and enabling quantile-specific inference on covariate effects.


**Acknowledgements: This report has been made with a template in R Markdown, taking as example the paper written by \citet{FanLi2001}. CHATGPT has been used for cleaning the code and debugging.**
