---
title: "Advance Regression Term Project"
author:
  - name: "Gerard Palomo & Juan Pablo Uphoff"
    affiliation:
      - affsuperscript: 1
        dptuniv: "Department of Statistics / University Carlos III of Madrid"
        address: >
          Calle Madrid 126 Getafe,
          28903,
          Spain.
#corrauthor:
#  url: https://www.uc3m.es
abstract: >
  Linear quantile regression extends ordinary least squares (OLS) by modeling conditional quantiles of a response variable as linear functions of predictors. This offers a more complete view of the conditional distribution, revealing heterogeneous effects not captured by mean regression. Unlike OLS, quantile regression makes no strict distributional assumptions and is robust to outliers. We review the formulation, estimation, and inference for linear quantile regression, contrasting it with OLS. A simulation study compares OLS and quantile regression at various quantile levels (0.1 to 0.9) in both univariate and multivariate settings. We examine performance under normal and heavy-tailed error distributions, including scenarios with outliers, and assess the impact of sample size. The results illustrate that quantile regression estimates remain reliable under outlier contamination and uncover distributional effects (e.g. heteroscedasticity) that OLS misses. All simulation code is provided in R for reproducibility.
keywords: [OLS, Quantile Regression, Machine Learning]
#JEL: [C45, C55]
acknowledgements: >
  This report was completed for the course **Advanced Regression and Prediction**, as part of the **MSc in Statistics for Data Science** at **University Carlos III of Madrid**.
journalinfo: "Working Paper, MSc in Statistics for Data Science, Carlos III University of Madrid"
archive: "DOI: N/A (internal report)"
date: "`r format(Sys.time(), '%Y %B %d')`"
lang: en-US
otherlangs: [fr-FR,it]
keywordlabel: Keywords
#JELlabel: JEL
#acknowledgementslabel: Acknowledgements
#corrauthorlabel: Corresponding author
bibliography: references.bib
biblio-style: apalike
toc-depth: 3
fontsize: 10pt
urlcolor: blue
preamble: >
  \hyphenation{quan-tile re-gres-sion out-li-ers het-ero-ske-das-tic}
always_allow_html: yes
csquotes: true
output:
  bookdown::pdf_book:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: yes
    toc: false
    number_sections: true
    fig_caption: true
    highlight: tango
    geometry: margin=1in
    linestretch: 1.5
    mainfont: "Times New Roman"  
    monofont: "Courier New"  
    template: latex/template.tex
---
```{r}
#| label: DoNotModify
#| include: false
### Utilities. Do not modify.
install_packages <- function(pkgs) {
  installed <- rownames(installed.packages())
  to_install <- setdiff(pkgs, installed)
  if (length(to_install) > 0) install.packages(to_install)
}
install_packages(c("bookdown", "tidyverse", "quantreg", "kableExtra", "gridExtra", "ragg"))

library(quantreg)
library(gridExtra)
library(kableExtra)
library(tidyverse)
```

```{r}
#| label: Options
#| include: false

### Customized options for this document
# Add necessary packages here
packages <- c("tidyverse")
# Install them
install_packages(packages)

# knitr options
knitr::opts_chunk$set(
  cache = FALSE, # Cache chunk results
  echo = FALSE, # Show/Hide code
  warning = FALSE, # Show/Hide warnings
  message = FALSE, # Show/Hide messages
  # Figure alignment and size
  fig.align = "center", out.width = "80%",
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
)
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)
knitr::opts_chunk$set(dev.args = list(bg = "transparent"))


```

```{r}
#| label: set up & simulation functions
#| include: false
#| echo: false

library(quantreg)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(kableExtra)
library(tibble)

set.seed(666)
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)

# Core functions for simulation, model fitting, and outlier injection
beta0 <- 5; beta1 <- 1.5
n_sim <- 100
outlier_frac <- 0.02
outlier_shift <- 50
tau_levels <- c(0.1, 0.5, 0.9)

simulate_data <- function(n = 200, heterosk = FALSE) {
  x1 <- runif(n, 0, 10)
  sd_e <- if (heterosk) (1 + 0.6 * x1) else 2
  e <- rnorm(n, mean = 0, sd = sd_e)
  y <- beta0 + beta1 * x1 + e
  data.frame(y = y, x1 = x1)
}

fit_models <- function(data) {
  ols <- lm(y ~ x1, data = data)
  qrs <- lapply(tau_levels, function(tau) rq(y ~ x1, tau = tau, data = data))
  list(ols = ols, qrs = qrs)
}

add_high_leverage_point <- function(data) {
  new_x1 <- max(data$x1) + 1
  new_y <- max(data$y) * 20
  new_point <- data.frame(y = new_y, x1 = new_x1)
  rbind(data, new_point)
}

```

# Introduction

Classical linear regression (ordinary least squares, OLS) fits a line by minimizing the sum of squared residuals, which targets the conditional mean of the response variable. However, because OLS puts heavy weight on large deviations, its estimates can be strongly affected by even a few outliers. In contrast, quantile regression extends median regression to any conditional quantile (e.g., the 50% quantile) and minimizes an asymmetrically weighted absolute loss. A key advantage of quantile regression is its robustness: the influence function of the median is bounded, so regression quantiles inherit a high breakdown point. In practical terms, estimates like the conditional median are not pulled as far by extreme values.

Prior studies confirm these properties: for example, Herawati (2020) showed in simulations that median regression provided smaller mean-squared error than OLS when outliers are present. In this article, we compare OLS and quantile regression for a linear model under controlled outlier contamination.

# Linear Quantile Regression: Theory and Methods

## Definition and Estimation

For a random variable $Y$, the $\tau$-th quantile is the value $q_\tau$ such that $P(Y \le q_\tau) = \tau$. In a regression setting, quantile regression (QR) estimates:

$$
Q_Y(\tau \mid X = x) = x^\top \beta(\tau),
$$

where $\beta(\tau)$ is a vector of coefficients specific to quantile level $\tau$. For example, $\beta_1(0.5)$ represents the effect of $X_1$ on the median of $Y$.

Koenker and Bassett (1978) proposed estimating $\beta(\tau)$ by minimizing the check loss:

$$
\hat\beta(\tau) = \arg\min_\beta \sum_{i=1}^n \rho_\tau(y_i - x_i^\top \beta),
$$

with $\rho_\tau(u) = u(\tau - \mathbb{I}\{u < 0\})$. For $\tau = 0.5$, this reduces to least absolute deviations (LAD) regression.

Each $\tau$ is estimated independently using linear programming, and the family $\{\beta(\tau)\}$ forms a quantile process describing the full conditional distribution of $Y$.

## Inference

Under regularity conditions, $\hat\beta(\tau)$ is asymptotically normal:

$$
\sqrt{n}(\hat\beta(\tau) - \beta(\tau)) \overset{d}{\to} N(0, \Sigma(\tau)),
$$

with variance estimators obtained via bootstrapping or sandwich estimators (Koenker, 2005). Practical inference is available via `summary()` in the R package `quantreg`.

## Comparison to OLS

OLS estimates the conditional mean:

$$
\mathbb{E}[Y \mid X = x] = x^\top \beta,
$$

whereas QR estimates conditional quantiles. When errors are symmetric and homoscedastic, QR and OLS give similar results. Otherwise, QR captures **distributional heterogeneity**, such as increasing variance or skewness.

Moreover, QR is **robust to outliers in $Y$**, unlike OLS which minimizes squared error and is sensitive to extreme values. QR also allows different slopes across quantiles, offering richer interpretation.

In the next section, we illustrate these theoretical advantages through a simulation study.


# Simulation Study

This section presents a simulation study designed to compare the performance of **Ordinary Least Squares (OLS)** and **Quantile Regression (QR)** estimators under controlled, interpretable scenarios. Our aim is to assess how both methods behave, particularly under outlier contamination.

To focus on the theoretical properties discussed in Section 2, we restrict attention to a simple univariate linear model with a single predictor. This allows for clean interpretation and visual representation of the results. Additionally, we consider only two error structures: a baseline homoscedastic Gaussian case and a contaminated version with outliers at high-leverage points. This controlled setup isolates the impact of extreme observations on both methods, and is helps to reveal the key differences in robustness and sensitivity. 

## Data Generating Process (DGP)

We consider the simple linear model \( Y = \beta_0 + \beta_1 X_1 + \varepsilon \), with fixed parameters \(\beta_0 = 5\) and \(\beta_1 = -1.5\). These values are chosen to induce a moderate negative slope and an interpretable intercept, ensuring that both OLS and QR coefficients remain in a tractable range for interpretation and graphical analysis. The predictor \(X_1\) is generated from a uniform distribution on \([0,10]\), which provides a constant density across its support and avoids introducing implicit bias or skewness into the covariate structure. The sample size \(n = 1000\) is selected to approximate asymptotic behavior while remaining computationally feasible. While we compute QR estimates for multiple quantiles (\(\tau \in \{0.1, 0.5, 0.9\}\)), the primary focus is on \(\tau = 0.5\), which corresponds to the conditional median and allows direct comparison with the OLS estimator of the conditional mean. As highlighted in Section 2, QR estimates \(\beta(\tau)\) independently for each \(\tau\), thus providing a richer description of the conditional distribution of \(Y\) than OLS.


## 3.2 Simulation Setup

To evaluate the estimators under realistic and adversarial conditions, we simulate two distinct error structures for \(\varepsilon\). First, we consider homoscedastic Gaussian errors: \(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\) with \(\sigma = 2\), which fulfill all Gauss-Markov conditions and provide a benchmark for both estimators. Second, we introduce contaminated Gaussian errors to test robustness: 2\% of the residuals are perturbed by a fixed additive shift (\(+50\)), applied to observations with the largest values of \(X_1\), thereby combining vertical outliers with high-leverage covariate values.

## 3.3 Evaluation Metrics

To quantify the behavior of the estimators, we combine visual inspection with numerical performance metrics. Given that QR minimizes absolute deviations, particularly for \(\tau = 0.5\), we employ the **Mean Absolute Error (MAE)** as the primary metric. MAE is defined as \(\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\), and aligns directly with the objective function of the LAD estimator. It provides a robust measure of predictive accuracy and is less sensitive to outliers than the **RMSE**, which disproportionately penalizes large deviations. This makes MAE more appropriate when comparing methods under contamination or heavy-tailed noise, as discussed by Koenker (2005).


## 3.4 Simulation
```{r}
#| label: generate-data-models
#| include: false

# --- Simulation Setup ---
set.seed(666) # Ensure reproducibility for simulation run
n <- 1000 # Sample size from DGP description
beta0 <- 5   # Intercept from simulation code
beta1 <- 1.5  # Slope from simulation code (NOTE: Text said -1.5, code uses 1.5)
outlier_frac <- 0.02
outlier_shift <- 50
tau_levels <- c(0.1, 0.5, 0.9) # Ensure this is defined

# --- Generate Data ---
# 1. Clean univariate data
data_clean_uni <- simulate_data(n = n, heterosk = FALSE)

# 2. Outlier univariate data
data_outlier_uni <- data_clean_uni
num_outliers <- round(outlier_frac * n)
# Add outliers to highest leverage points (largest x1 values)
out_idx <- order(data_outlier_uni$x1, decreasing = TRUE)[1:num_outliers]
data_outlier_uni$y[out_idx] <- data_outlier_uni$y[out_idx] + outlier_shift

# --- Store Data in 'scenarios' list ---
scenarios <- list(
  clean_uni = data_clean_uni,
  outlier_uni = data_outlier_uni
)

# --- Fit Models ---
models <- list(
  clean_uni = fit_models(scenarios$clean_uni),
  outlier_uni = fit_models(scenarios$outlier_uni)
)
```

```{r boxplots-2x2, fig.width=9, fig.height=6, fig.cap="Comparison of slope estimates across settings"}
library(gridExtra)

plot_one <- function(data, model, title) {
  ggplot(data, aes(x = x1, y = y)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
    geom_quantile(quantiles = tau_levels, color = "blue", size = 0.8) +
    ggtitle(title) +
    theme_minimal()
}

grid.arrange(
  plot_one(scenarios$clean_uni, models$clean_uni, "Clean Univariate"),
  plot_one(scenarios$outlier_uni, models$outlier_uni, "Outlier Univariate"),
  plot_one(scenarios$clean_multi, models$clean_multi, "Clean Multivariate"),
  plot_one(scenarios$outlier_multi, models$outlier_multi, "Outlier Multivariate"),
  ncol = 2
)
```

```{r slope-summary-table, results='asis'}
summarize_slopes <- function(model) {
  slopes <- sapply(model$qrs, function(m) coef(m)["x1"])
  tibble(
    Method = c(paste0("QR (tau=", tau_levels, ")"), "OLS"),
    Estimate = c(slopes, coef(model$ols)["x1"])
  )
}

summarize_slopes(models$clean_uni)
```

```{r plot-coef-vs-quantile, fig.cap="Slope vs Quantile for heteroskedastic case"}
set.seed(666)
data_heterosk <- simulate_data(n=1000, heterosk=TRUE)
fits_heterosk <- lapply(seq(0.05, 0.95, 0.05), function(tau) rq(y ~ x1, tau = tau, data = data_heterosk))
slopes_heterosk <- sapply(fits_heterosk, function(fit) coef(fit)[2])
ols_slope <- coef(lm(y ~ x1, data = data_heterosk))[2]

plot_df <- tibble(
  tau = seq(0.05, 0.95, 0.05),
  slope = slopes_heterosk
)

ggplot(plot_df, aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  geom_hline(yintercept = ols_slope, color = "red", linetype = "dashed") +
  labs(x = "Quantile (tau)", y = "Slope", title = "Slope vs Quantile Level") +
  theme_minimal()
```
```{r anova-quantiles}
fit25 <- rq(y ~ x1, tau=0.25, data=scenarios$clean_uni)
fit50 <- rq(y ~ x1, tau=0.5, data=scenarios$clean_uni)
fit75 <- rq(y ~ x1, tau=0.75, data=scenarios$clean_uni)

anova(fit25, fit50, fit75)
#kbl(anova_table, caption = "Joint Test of Equality of Slopes at Different Quantiles") %>%
#  kable_styling(latex_options = "striped")
```

```{r other-plots}
library(quantreg)
library(gridExtra)
library(dplyr)

# Simulate basic heteroskedastic data
set.seed(666)
n <- 500
X <- runif(n, 0, 10)
epsilon <- rnorm(n, mean = 0, sd = 1 + 0.3 * X)
Y <- 5 + 1.5 * X + epsilon
data <- data.frame(X = X, Y = Y)

# Fit models
ols_fit <- lm(Y ~ X, data = data)
rq_10 <- rq(Y ~ X, tau = 0.1, data = data)
rq_50 <- rq(Y ~ X, tau = 0.5, data = data)
rq_90 <- rq(Y ~ X, tau = 0.9, data = data)

# Plot 1: Scatterplot + regression lines
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_quantile(quantiles = c(0.1, 0.5, 0.9), color = "blue", size = 0.8) +
  labs(title = "OLS vs Quantile Regression Lines", y = "Y", x = "X") +
  theme_minimal()

# Plot 2: Slope across quantiles
taus <- seq(0.05, 0.95, by = 0.05)
slopes <- sapply(taus, function(tau) coef(rq(Y ~ X, tau = tau))[2])
p2 <- ggplot(data.frame(tau = taus, slope = slopes), aes(x = tau, y = slope)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = coef(ols_fit)[2], linetype = "dashed", color = "red") +
  labs(title = "Slope estimates across quantiles", y = "Slope", x = "Quantile (tau)") +
  theme_minimal()

# Plot 3: Robustness to outliers
data_outlier <- data
idx <- sample(1:n, 10)
data_outlier$Y[idx] <- data_outlier$Y[idx] + 50  # Add strong outliers

ols_fit_outlier <- lm(Y ~ X, data = data_outlier)
rq_50_outlier <- rq(Y ~ X, tau = 0.5, data = data_outlier)

p3 <- ggplot(data_outlier, aes(x = X, y = Y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_abline(intercept = coef(rq_50_outlier)[1], slope = coef(rq_50_outlier)[2], color = "blue") +
  labs(title = "Effect of Outliers: OLS vs Median QR", y = "Y", x = "X") +
  theme_minimal()

# Plot 4: Residuals histograms
residuals_ols <- resid(ols_fit)
residuals_rq50 <- resid(rq_50)

residuals_df <- data.frame(
  residuals = c(residuals_ols, residuals_rq50),
  Method = rep(c("OLS", "QR (tau=0.5)"), each = n)
)

p4 <- ggplot(residuals_df, aes(x = residuals, fill = Method)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap(~Method) +
  labs(title = "Residuals Distribution: OLS vs QR", x = "Residuals", y = "Count") +
  theme_minimal()

# Arrange in grid (2x2 layout)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```


# Discussion & Conclusions. 

xxxxx

***Extras and considerations***: This report has been made with a template in R Markdown, following the guidelines from [@Marcon2025], and the most famous resource for the creation of reports in R Markdown, the bookdown package [@Xie2016].
